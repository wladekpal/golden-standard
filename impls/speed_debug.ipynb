{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "# os.chdir('/home/mbortkie/repos/crl_subgoal')\n",
    "import wandb\n",
    "\n",
    "from rb import TrajectoryUniformSamplingQueue, jit_wrap, segment_ids_per_row\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from typing import Tuple, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import chex\n",
    "from flax import struct\n",
    "from absl import app, flags\n",
    "from ml_collections import config_flags\n",
    "from agents import agents\n",
    "from agents.crl import CRLAgent, get_config\n",
    "from config import ROOT_DIR\n",
    "from block_moving_env import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap environment\n",
    "NUM_ENVS = 1024\n",
    "MAX_REPLAY_SIZE = 10000\n",
    "BATCH_SIZE = 1024\n",
    "EPISODE_LENGTH = 100\n",
    "NUM_ACTIONS = 6\n",
    "GRID_SIZE = 5\n",
    "NUM_BOXES = 1\n",
    "SEED = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BoxPushingEnv(grid_size=GRID_SIZE, max_steps=EPISODE_LENGTH, number_of_boxes=NUM_BOXES)\n",
    "env = AutoResetWrapper(env)\n",
    "key = random.PRNGKey(SEED)\n",
    "env.step = jax.jit(jax.vmap(env.step))\n",
    "env.reset = jax.jit(jax.vmap(env.reset))\n",
    "jitted_flatten_batch = jax.jit(jax.vmap(flatten_batch, in_axes=(None, 0, 0)), static_argnums=(0,))\n",
    "\n",
    "# Replay buffer\n",
    "dummy_timestep = TimeStep(\n",
    "    key=key,\n",
    "    grid=jnp.zeros((GRID_SIZE, GRID_SIZE), dtype=jnp.int32),\n",
    "    target_cells=jnp.zeros((NUM_BOXES, 2), dtype=jnp.int32),\n",
    "    agent_pos=jnp.zeros((2,), dtype=jnp.int32),\n",
    "    agent_has_box=jnp.zeros((1,), dtype=jnp.int32),\n",
    "    steps=jnp.zeros((1,), dtype=jnp.int32),\n",
    "    action=jnp.zeros((1,), dtype=jnp.int32),\n",
    "    goal=jnp.zeros((GRID_SIZE, GRID_SIZE), dtype=jnp.int32),\n",
    "    reward=jnp.zeros((1,), dtype=jnp.int32),\n",
    ")\n",
    "replay_buffer = jit_wrap(\n",
    "    TrajectoryUniformSamplingQueue(\n",
    "        max_replay_size=MAX_REPLAY_SIZE,\n",
    "        dummy_data_sample=dummy_timestep,\n",
    "        sample_batch_size=BATCH_SIZE,\n",
    "        num_envs=NUM_ENVS,\n",
    "        episode_length=EPISODE_LENGTH,\n",
    "    )\n",
    ")\n",
    "buffer_state = jax.jit(replay_buffer.init)(key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing agent creation\n",
      "Agent created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Agent\n",
    "config = get_config()\n",
    "config['discrete'] = True\n",
    "agent_class = agents[config['agent_name']]\n",
    "example_batch = {\n",
    "    'observations':dummy_timestep.grid.reshape(1, -1),  # Add batch dimension \n",
    "    'actions': jnp.ones((1,), dtype=jnp.int32) * (NUM_ACTIONS-1), # TODO: make sure it should be the maximal value of action space  # Single action for batch size 1\n",
    "    'value_goals': dummy_timestep.grid.reshape(1, -1),\n",
    "    'actor_goals': dummy_timestep.grid.reshape(1, -1),\n",
    "}\n",
    "\n",
    "print(\"Testing agent creation\")\n",
    "agent = agent_class.create(\n",
    "    SEED,\n",
    "    example_batch['observations'],\n",
    "    example_batch['actions'],\n",
    "    config,\n",
    "    example_batch['value_goals'],\n",
    ")\n",
    "print(\"Agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collection_key = random.PRNGKey(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_step(carry, _):\n",
    "    buffer_state, agent, key = carry\n",
    "    key, batch_key, subkey1, subkey2 = jax.random.split(key, 4)\n",
    "    # Sample and process transitions\n",
    "    buffer_state, transitions = replay_buffer.sample(buffer_state)\n",
    "    batch_keys = jax.random.split(batch_key, transitions.grid.shape[0])\n",
    "    state, future_state, goal_index = jitted_flatten_batch(0.99, transitions, batch_keys)\n",
    "\n",
    "    # Get random index for each batch\n",
    "    random_indices = jax.random.randint(subkey1, (state.grid.shape[0],), minval=0, maxval=state.grid.shape[1])            \n",
    "\n",
    "    # Extract data at random index\n",
    "    state1 = jax.tree_util.tree_map(lambda x: x[jnp.arange(x.shape[0]), random_indices], state)\n",
    "    actions = state1.action\n",
    "    future_state1 = jax.tree_util.tree_map(lambda x: x[jnp.arange(x.shape[0]), random_indices], future_state)\n",
    "    goal_index1 = jax.tree_util.tree_map(lambda x: x[jnp.arange(x.shape[0]), random_indices], goal_index)\n",
    "    \n",
    "    random_indices2 = jax.random.randint(subkey2, (state.grid.shape[0],), minval=0, maxval=state.grid.shape[1])            \n",
    "\n",
    "    # Extract data at random index\n",
    "    state2 = jax.tree_util.tree_map(lambda x: x[jnp.arange(x.shape[0]), random_indices2], state)\n",
    "    actions2 = state2.action\n",
    "    future_state2 = jax.tree_util.tree_map(lambda x: x[jnp.arange(x.shape[0]), random_indices2], future_state)\n",
    "    goal_index2 = jax.tree_util.tree_map(lambda x: x[jnp.arange(x.shape[0]), random_indices2], goal_index)\n",
    "\n",
    "    state = jax.tree_util.tree_map(lambda x1, x2: jnp.concatenate([x1, x2], axis=0), state1, state2)\n",
    "    actions = jnp.concatenate([actions, actions2], axis=0)\n",
    "    future_state = jax.tree_util.tree_map(lambda x1, x2: jnp.concatenate([x1, x2], axis=0), future_state1, future_state2)\n",
    "    goal_index = jnp.concatenate([goal_index1, goal_index2], axis=0)\n",
    "    \n",
    "    # Create valid batch\n",
    "    valid_batch = {\n",
    "        'observations': state.grid.reshape(state.grid.shape[0], -1),\n",
    "        'actions': actions.squeeze(),\n",
    "        'value_goals': future_state.grid.reshape(future_state.grid.shape[0], -1),\n",
    "        'actor_goals': future_state.grid.reshape(future_state.grid.shape[0], -1),\n",
    "    }\n",
    "\n",
    "    # Update agent\n",
    "    agent, update_info = agent.update(valid_batch)\n",
    "    return (buffer_state, agent, key), update_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_epoch(carry, _):\n",
    "  buffer_state, agent, key = carry\n",
    "  key, data_key, up_key = jax.random.split(key, 3)\n",
    "  _, _, timesteps = collect_data(agent, data_key, env, NUM_ENVS, EPISODE_LENGTH)\n",
    "  buffer_state = replay_buffer.insert(buffer_state, timesteps)\n",
    "  (buffer_state, agent, _), _ = jax.lax.scan(update_step, (buffer_state, agent, up_key), None, length=1000)\n",
    "  return (buffer_state, agent, key), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "(buffer_state, agent, key), _ = train_epoch((buffer_state, agent, key), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# then outside:\n",
    "(buffer_state, agent, key), _ = jax.lax.scan(train_epoch, (buffer_state, agent, key), None, length=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    (buffer_state, agent, key), _ = train_epoch((buffer_state, agent, key), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_n_epochs(buffer_state, agent, key):\n",
    "    (buffer_state, agent, key), _ = jax.lax.scan(\n",
    "        train_epoch,\n",
    "        (buffer_state, agent, key),\n",
    "        None,\n",
    "        length=10,\n",
    "    )\n",
    "    return buffer_state, agent, key\n",
    "\n",
    "# first call pays compilation cost\n",
    "(buffer_state, agent, key) = train_n_epochs(buffer_state, agent, key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsequent calls are ~0.05 s\n",
    "for _ in range(10):\n",
    "    buffer_state, agent, key = train_n_epochs(buffer_state, agent, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(10):\n",
    "    key, data_collection_key, update_key = jax.random.split(key, 3)\n",
    "    env_step, info, timesteps_all = collect_data(agent, data_collection_key, env, NUM_ENVS, EPISODE_LENGTH)\n",
    "    buffer_state = replay_buffer.insert(buffer_state, timesteps_all)\n",
    "    \n",
    "    # Run scan for updates\n",
    "    (buffer_state, agent, _), update_infos = jax.lax.scan(\n",
    "        update_step,\n",
    "        (buffer_state, agent, update_key),\n",
    "        None,\n",
    "        length=10\n",
    "    )\n",
    "\n",
    "    update_infos = jax.tree_util.tree_map(lambda x: x[-1], update_infos)\n",
    "    \n",
    "    update_infos.update({\n",
    "        \"eval/reward_min\": timesteps_all.reward.min(),\n",
    "        \"eval/reward_max\": timesteps_all.reward.max(), \n",
    "        \"eval/reward_mean\": timesteps_all.reward.mean()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 100, 5, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer_state, batch = replay_buffer.sample(buffer_state)\n",
    "batch.grid.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 100, 5, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps_all.grid.shape\n",
    "timesteps_all.grid.swapaxes(1, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def extract_at_indices(data, indices):\n",
    "    return jax.tree_util.tree_map(lambda x: x[jnp.arange(x.shape[0]), indices], data)\n",
    "\n",
    "@jax.jit\n",
    "def apply_double_batch_trick(state, future_state, goal_index, key):\n",
    "    \"\"\"Sample two random indices and concatenate the results.\"\"\"\n",
    "    # Sample two random indices for each batch\n",
    "    subkey1, subkey2 = jax.random.split(key, 2)\n",
    "    random_indices1 = jax.random.randint(subkey1, (state.grid.shape[0],), minval=0, maxval=state.grid.shape[1])\n",
    "    random_indices2 = jax.random.randint(subkey2, (state.grid.shape[0],), minval=0, maxval=state.grid.shape[1])\n",
    "\n",
    "    state1 = extract_at_indices(state, random_indices1)\n",
    "    state2 = extract_at_indices(state, random_indices2)\n",
    "    future_state1 = extract_at_indices(future_state, random_indices1)\n",
    "    future_state2 = extract_at_indices(future_state, random_indices2)\n",
    "    goal_index1 = extract_at_indices(goal_index, random_indices1)\n",
    "    goal_index2 = extract_at_indices(goal_index, random_indices2)\n",
    "    \n",
    "    # Concatenate the two samples\n",
    "    state = jax.tree_util.tree_map(lambda x1, x2: jnp.concatenate([x1, x2], axis=0), state1, state2)\n",
    "    actions = jnp.concatenate([state1.action, state2.action], axis=0)\n",
    "    future_state = jax.tree_util.tree_map(lambda x1, x2: jnp.concatenate([x1, x2], axis=0), future_state1, future_state2)\n",
    "    goal_index = jnp.concatenate([goal_index1, goal_index2], axis=0)\n",
    "    \n",
    "    return state, actions, future_state, goal_index\n",
    "\n",
    "def evaluate_agent(agent, env, key, num_envs=1024, episode_length=100):\n",
    "    \"\"\"Evaluate agent by running rollouts using collect_data and computing losses.\"\"\"\n",
    "    key, data_key, double_batch_key = jax.random.split(key, 3)\n",
    "    # Use collect_data for evaluation rollouts\n",
    "    _, _, timesteps = collect_data(agent, data_key, env, num_envs, episode_length)\n",
    "    timesteps = jax.tree_util.tree_map(lambda x: x.swapaxes(1, 0), timesteps)\n",
    "\n",
    "    batch_keys = jax.random.split(data_key, num_envs)\n",
    "    state, future_state, goal_index = jitted_flatten_batch(0.99, timesteps, batch_keys)\n",
    "    \n",
    "    # Sample and concatenate batch using the new function\n",
    "    state, actions, future_state, goal_index = apply_double_batch_trick(state, future_state, goal_index, double_batch_key)\n",
    "    \n",
    "    # Create valid batch\n",
    "    valid_batch = {\n",
    "        'observations': state.grid.reshape(state.grid.shape[0], -1),\n",
    "        'actions': actions.squeeze(),\n",
    "        'value_goals': future_state.grid.reshape(future_state.grid.shape[0], -1),\n",
    "        'actor_goals': future_state.grid.reshape(future_state.grid.shape[0], -1),\n",
    "    }\n",
    "\n",
    "    # Compute losses on example batch\n",
    "    loss, loss_info = agent.total_loss(valid_batch, None)\n",
    "    \n",
    "    # Compile evaluation info\n",
    "    eval_info = {\n",
    "        'eval/mean_reward': timesteps.reward.mean(),\n",
    "        'eval/min_reward': timesteps.reward.min(),\n",
    "        'eval/max_reward': timesteps.reward.max(),\n",
    "        'eval/total_loss': loss,\n",
    "    }\n",
    "    eval_info.update(loss_info)\n",
    "    \n",
    "    return eval_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 5, 5)\n",
      "Evaluation Results:\n",
      "eval/mean_reward: 0.07830078154802322\n",
      "eval/min_reward: 0\n",
      "eval/max_reward: 1\n",
      "eval/total_loss: 1.5962635278701782\n",
      "actor/actor_loss: 1.5877385139465332\n",
      "actor/adv: -0.44561105966567993\n",
      "actor/bc_log_prob: -1.6485228538513184\n",
      "critic/binary_accuracy: 0.99951171875\n",
      "critic/categorical_accuracy: 0.0029296875\n",
      "critic/contrastive_loss: 0.004223821219056845\n",
      "critic/logits: -8.592992782592773\n",
      "critic/logits_neg: -8.593518257141113\n",
      "critic/logits_pos: -7.519384384155273\n",
      "critic/v_max: 0.0724712684750557\n",
      "critic/v_mean: 0.002251780591905117\n",
      "critic/v_min: 2.553087369960849e-06\n",
      "value/binary_accuracy: 0.99951171875\n",
      "value/categorical_accuracy: 0.0029296875\n",
      "value/contrastive_loss: 0.004301227163523436\n",
      "value/logits: -8.580251693725586\n",
      "value/logits_neg: -8.580719947814941\n",
      "value/logits_pos: -7.618547439575195\n",
      "value/v_max: 0.04266826808452606\n",
      "value/v_mean: 0.0023576815146952868\n",
      "value/v_min: 1.0530050076340558e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_agent(agent, env, key, 1024, 100)\n",
    "print(\"Evaluation Results:\")\n",
    "for k, v in eval_results.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = evaluate_agent(agent, env, key, 1024, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
